{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aoxODC3qIl7",
        "outputId": "3df79fcf-7bb5-47de-f8fd-f4a3c1e9097d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.20.0-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m121.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydub, xxhash, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, dill, aiofiles, starlette, multiprocess, safehttpx, gradio-client, fastapi, gradio, datasets\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 datasets-3.3.2 dill-0.3.8 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.0 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 multiprocess-0.70.16 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.0 tomlkit-0.13.2 uvicorn-0.34.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.0.1 torchtext==0.15.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBDxfZ_NqS27",
        "outputId": "f78159bc-f4bd-4022-96b7-7f5bad930aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchtext==0.15.2\n",
            "  Downloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (1.26.4)\n",
            "Collecting torchdata==0.6.1 (from torchtext==0.15.2)\n",
            "  Downloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.1.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.3.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.4)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchtext-0.15.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lit, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed lit-18.1.8 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 torchdata-0.6.1 torchtext-0.15.2 triton-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmpc30L4qmAx",
        "outputId": "aadc82ef-3232-4538-af4c-b9e5d274158d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import gradio as gr\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('Helsinki-NLP/tatoeba_mt', 'ara-eng')\n",
        "\n",
        "# Basic tokenization function (word-level for simplicity)\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "\n",
        "# Build vocabulary from dataset\n",
        "def build_vocab(data, tokenizer, min_freq=2):\n",
        "    counter = Counter()\n",
        "    for example in data:\n",
        "        counter.update(tokenizer(example['sourceString']))\n",
        "        counter.update(tokenizer(example['targetString']))\n",
        "    # Add special tokens\n",
        "    specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "    vocab = specials + [word for word, freq in counter.items() if freq >= min_freq]\n",
        "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
        "    return word2idx, idx2word\n",
        "\n",
        "# Convert text to tensor (adjusted to fit special tokens within max_len)\n",
        "def text_to_tensor(text, vocab, tokenizer, max_len=52):\n",
        "    tokens = tokenizer(text)[:max_len - 2]  # Reserve space for <sos> and <eos>\n",
        "    tokens = ['<sos>'] + tokens + ['<eos>']\n",
        "    tensor = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    return torch.tensor(tensor, dtype=torch.long)\n",
        "\n",
        "# Prepare dataset for training\n",
        "train_data = dataset['validation']  # Using validation as training data for demo\n",
        "test_data = dataset['test']\n",
        "\n",
        "# Build shared vocabulary (for simplicity, using both languages in one vocab)\n",
        "word2idx, idx2word = build_vocab(train_data, tokenize)\n",
        "\n",
        "# Hyperparameters for data\n",
        "max_len = 52  # Increased to account for <sos> and <eos>\n",
        "batch_size = 32\n",
        "\n",
        "# Simple data loader (debugged)\n",
        "train_data_list = list(train_data)  # Convert Dataset to list once\n",
        "print(f\"Length of train_data_list: {len(train_data_list)}\")  # Should be 19528\n",
        "\n",
        "def get_batches(data_list, batch_size, max_len=52):\n",
        "    total_batches = len(data_list) // batch_size + (1 if len(data_list) % batch_size else 0)\n",
        "    print(f\"Total batches to process: {total_batches}\")\n",
        "    for i in range(0, len(data_list), batch_size):\n",
        "        batch = data_list[i:i + batch_size]\n",
        "        src_batch = [text_to_tensor(example['sourceString'], word2idx, tokenize, max_len) for example in batch]\n",
        "        tgt_batch = [text_to_tensor(example['targetString'], word2idx, tokenize, max_len) for example in batch]\n",
        "        src_batch = nn.utils.rnn.pad_sequence(src_batch, padding_value=word2idx['<pad>'], batch_first=False).to(device)\n",
        "        tgt_batch = nn.utils.rnn.pad_sequence(tgt_batch, padding_value=word2idx['<pad>'], batch_first=False).to(device)\n",
        "        if src_batch.size(0) > max_len:\n",
        "            src_batch = src_batch[:max_len, :]\n",
        "        elif src_batch.size(0) < max_len:\n",
        "            padding = torch.full((max_len - src_batch.size(0), src_batch.size(1)), word2idx['<pad>'], dtype=torch.long).to(device)\n",
        "            src_batch = torch.cat([src_batch, padding], dim=0)\n",
        "        if tgt_batch.size(0) > max_len:\n",
        "            tgt_batch = tgt_batch[:max_len, :]\n",
        "        elif tgt_batch.size(0) < max_len:\n",
        "            padding = torch.full((max_len - tgt_batch.size(0), tgt_batch.size(1)), word2idx['<pad>'], dtype=torch.long).to(device)\n",
        "            tgt_batch = torch.cat([tgt_batch, padding], dim=0)\n",
        "        src_batch = src_batch.transpose(0, 1)  # [batch_size, seq_len]\n",
        "        tgt_batch = tgt_batch.transpose(0, 1)  # [batch_size, seq_len]\n",
        "        yield src_batch, tgt_batch\n",
        "\n",
        "\n",
        "print(\"Revised Chunk 1 (Seventh Iteration) completed: Dataset loaded and preprocessing debugged.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT320YaDqXpA",
        "outputId": "3d2b2d53-ad3d-4911-9d83-fcae0f9aac27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train_data_list: 19528\n",
            "Revised Chunk 1 (Seventh Iteration) completed: Dataset loaded and preprocessing debugged.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding (fixed max_len to 52)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=52):  # Updated to 52\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)  # Shape: (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "# Multi-Head Attention (unchanged)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, V)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        seq_len_q = Q.size(1)\n",
        "        seq_len_k = K.size(1)\n",
        "        Q = self.W_q(Q)\n",
        "        K = self.W_k(K)\n",
        "        V = self.W_v(V)\n",
        "        Q = Q.view(batch_size, seq_len_q, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = K.view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = V.view(batch_size, seq_len_k, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, self.d_model)\n",
        "        return self.W_o(output)\n",
        "\n",
        "# Feed-Forward Network\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "# Encoder Layer\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "# Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn1_output = self.mha1(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn1_output))\n",
        "        attn2_output = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn2_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "# Full Transformer Model (fixed max_len to 52)\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8, num_layers=3, d_ff=1024, max_len=52, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != word2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != word2idx['<pad>']).unsqueeze(1).unsqueeze(3)\n",
        "        seq_len = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool().to(device)\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))\n",
        "        tgt_embedded = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        return self.fc_out(dec_output)\n",
        "\n",
        "print(\"Revised Chunk 2 (Fourth Iteration) completed: Transformer model fixed with max_len=52.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyPdrhT2qYwc",
        "outputId": "5fcdd91c-00e9-4219-f9f4-60cb6cae48c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Revised Chunk 2 (Fourth Iteration) completed: Transformer model fixed with max_len=52.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model instantiation\n",
        "vocab_size = len(word2idx)\n",
        "model = Transformer(\n",
        "    src_vocab_size=vocab_size,\n",
        "    tgt_vocab_size=vocab_size,\n",
        "    d_model=256,\n",
        "    num_heads=8,\n",
        "    num_layers=3,\n",
        "    d_ff=1024,\n",
        "    max_len=52,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop with progress feedback\n",
        "def train(model, data, epochs=20):\n",
        "    model.train()\n",
        "    total_batches = len(data) // batch_size + (1 if len(data) % batch_size else 0)\n",
        "    print(f\"Total batches per epoch: {total_batches}\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (src_batch, tgt_batch) in enumerate(get_batches(data, batch_size, max_len=52), 1):\n",
        "            if batch_idx % 100 == 0:  # Print every 100 batches for feedback\n",
        "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}/{total_batches} \")\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_batch, tgt_batch[:, :-1])\n",
        "            loss = criterion(output.view(-1, vocab_size), tgt_batch[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Inference function\n",
        "def translate(model, sentence, max_len=52):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = text_to_tensor(sentence, word2idx, tokenize, max_len).unsqueeze(0).to(device)\n",
        "        tgt = torch.tensor([word2idx['<sos>']], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        for _ in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
        "            if next_token == word2idx['<eos>']:\n",
        "                break\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "        translated = [idx2word[idx.item()] for idx in tgt[0] if idx.item() in idx2word]\n",
        "        return ' '.join(translated[1:])\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "train(model, train_data_list)\n",
        "print(\"Training completed.\")\n",
        "\n",
        "# Test inference\n",
        "test_sentence = \"عمرك رايح المكسيك؟\"\n",
        "translated = translate(model, test_sentence)\n",
        "print(f\"Input: {test_sentence}\")\n",
        "print(f\"Translated: {translated}\")\n",
        "\n",
        "print(\"Chunk 3 completed: Training and inference implemented.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "xCeVlZJHqcf2",
        "outputId": "6518febf-b9cd-495e-8e4a-4530f98b7dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Total batches per epoch: 611\n",
            "Total batches to process: 611\n",
            "Epoch 1, Batch 100/611 \n",
            "Epoch 1, Batch 200/611 \n",
            "Epoch 1, Batch 300/611 \n",
            "Epoch 1, Batch 400/611 \n",
            "Epoch 1, Batch 500/611 \n",
            "Epoch 1, Batch 600/611 \n",
            "Epoch 1/20, Loss: 6.1513\n",
            "Total batches to process: 611\n",
            "Epoch 2, Batch 100/611 \n",
            "Epoch 2, Batch 200/611 \n",
            "Epoch 2, Batch 300/611 \n",
            "Epoch 2, Batch 400/611 \n",
            "Epoch 2, Batch 500/611 \n",
            "Epoch 2, Batch 600/611 \n",
            "Epoch 2/20, Loss: 5.1121\n",
            "Total batches to process: 611\n",
            "Epoch 3, Batch 100/611 \n",
            "Epoch 3, Batch 200/611 \n",
            "Epoch 3, Batch 300/611 \n",
            "Epoch 3, Batch 400/611 \n",
            "Epoch 3, Batch 500/611 \n",
            "Epoch 3, Batch 600/611 \n",
            "Epoch 3/20, Loss: 4.6931\n",
            "Total batches to process: 611\n",
            "Epoch 4, Batch 100/611 \n",
            "Epoch 4, Batch 200/611 \n",
            "Epoch 4, Batch 300/611 \n",
            "Epoch 4, Batch 400/611 \n",
            "Epoch 4, Batch 500/611 \n",
            "Epoch 4, Batch 600/611 \n",
            "Epoch 4/20, Loss: 4.3833\n",
            "Total batches to process: 611\n",
            "Epoch 5, Batch 100/611 \n",
            "Epoch 5, Batch 200/611 \n",
            "Epoch 5, Batch 300/611 \n",
            "Epoch 5, Batch 400/611 \n",
            "Epoch 5, Batch 500/611 \n",
            "Epoch 5, Batch 600/611 \n",
            "Epoch 5/20, Loss: 4.1299\n",
            "Total batches to process: 611\n",
            "Epoch 6, Batch 100/611 \n",
            "Epoch 6, Batch 200/611 \n",
            "Epoch 6, Batch 300/611 \n",
            "Epoch 6, Batch 400/611 \n",
            "Epoch 6, Batch 500/611 \n",
            "Epoch 6, Batch 600/611 \n",
            "Epoch 6/20, Loss: 3.9104\n",
            "Total batches to process: 611\n",
            "Epoch 7, Batch 100/611 \n",
            "Epoch 7, Batch 200/611 \n",
            "Epoch 7, Batch 300/611 \n",
            "Epoch 7, Batch 400/611 \n",
            "Epoch 7, Batch 500/611 \n",
            "Epoch 7, Batch 600/611 \n",
            "Epoch 7/20, Loss: 3.7017\n",
            "Total batches to process: 611\n",
            "Epoch 8, Batch 100/611 \n",
            "Epoch 8, Batch 200/611 \n",
            "Epoch 8, Batch 300/611 \n",
            "Epoch 8, Batch 400/611 \n",
            "Epoch 8, Batch 500/611 \n",
            "Epoch 8, Batch 600/611 \n",
            "Epoch 8/20, Loss: 3.5075\n",
            "Total batches to process: 611\n",
            "Epoch 9, Batch 100/611 \n",
            "Epoch 9, Batch 200/611 \n",
            "Epoch 9, Batch 300/611 \n",
            "Epoch 9, Batch 400/611 \n",
            "Epoch 9, Batch 500/611 \n",
            "Epoch 9, Batch 600/611 \n",
            "Epoch 9/20, Loss: 3.3238\n",
            "Total batches to process: 611\n",
            "Epoch 10, Batch 100/611 \n",
            "Epoch 10, Batch 200/611 \n",
            "Epoch 10, Batch 300/611 \n",
            "Epoch 10, Batch 400/611 \n",
            "Epoch 10, Batch 500/611 \n",
            "Epoch 10, Batch 600/611 \n",
            "Epoch 10/20, Loss: 3.1490\n",
            "Total batches to process: 611\n",
            "Epoch 11, Batch 100/611 \n",
            "Epoch 11, Batch 200/611 \n",
            "Epoch 11, Batch 300/611 \n",
            "Epoch 11, Batch 400/611 \n",
            "Epoch 11, Batch 500/611 \n",
            "Epoch 11, Batch 600/611 \n",
            "Epoch 11/20, Loss: 2.9762\n",
            "Total batches to process: 611\n",
            "Epoch 12, Batch 100/611 \n",
            "Epoch 12, Batch 200/611 \n",
            "Epoch 12, Batch 300/611 \n",
            "Epoch 12, Batch 400/611 \n",
            "Epoch 12, Batch 500/611 \n",
            "Epoch 12, Batch 600/611 \n",
            "Epoch 12/20, Loss: 2.8136\n",
            "Total batches to process: 611\n",
            "Epoch 13, Batch 100/611 \n",
            "Epoch 13, Batch 200/611 \n",
            "Epoch 13, Batch 300/611 \n",
            "Epoch 13, Batch 400/611 \n",
            "Epoch 13, Batch 500/611 \n",
            "Epoch 13, Batch 600/611 \n",
            "Epoch 13/20, Loss: 2.6486\n",
            "Total batches to process: 611\n",
            "Epoch 14, Batch 100/611 \n",
            "Epoch 14, Batch 200/611 \n",
            "Epoch 14, Batch 300/611 \n",
            "Epoch 14, Batch 400/611 \n",
            "Epoch 14, Batch 500/611 \n",
            "Epoch 14, Batch 600/611 \n",
            "Epoch 14/20, Loss: 2.4945\n",
            "Total batches to process: 611\n",
            "Epoch 15, Batch 100/611 \n",
            "Epoch 15, Batch 200/611 \n",
            "Epoch 15, Batch 300/611 \n",
            "Epoch 15, Batch 400/611 \n",
            "Epoch 15, Batch 500/611 \n",
            "Epoch 15, Batch 600/611 \n",
            "Epoch 15/20, Loss: 2.3394\n",
            "Total batches to process: 611\n",
            "Epoch 16, Batch 100/611 \n",
            "Epoch 16, Batch 200/611 \n",
            "Epoch 16, Batch 300/611 \n",
            "Epoch 16, Batch 400/611 \n",
            "Epoch 16, Batch 500/611 \n",
            "Epoch 16, Batch 600/611 \n",
            "Epoch 16/20, Loss: 2.1968\n",
            "Total batches to process: 611\n",
            "Epoch 17, Batch 100/611 \n",
            "Epoch 17, Batch 200/611 \n",
            "Epoch 17, Batch 300/611 \n",
            "Epoch 17, Batch 400/611 \n",
            "Epoch 17, Batch 500/611 \n",
            "Epoch 17, Batch 600/611 \n",
            "Epoch 17/20, Loss: 2.0583\n",
            "Total batches to process: 611\n",
            "Epoch 18, Batch 100/611 \n",
            "Epoch 18, Batch 200/611 \n",
            "Epoch 18, Batch 300/611 \n",
            "Epoch 18, Batch 400/611 \n",
            "Epoch 18, Batch 500/611 \n",
            "Epoch 18, Batch 600/611 \n",
            "Epoch 18/20, Loss: 1.9222\n",
            "Total batches to process: 611\n",
            "Epoch 19, Batch 100/611 \n",
            "Epoch 19, Batch 200/611 \n",
            "Epoch 19, Batch 300/611 \n",
            "Epoch 19, Batch 400/611 \n",
            "Epoch 19, Batch 500/611 \n",
            "Epoch 19, Batch 600/611 \n",
            "Epoch 19/20, Loss: 1.7939\n",
            "Total batches to process: 611\n",
            "Epoch 20, Batch 100/611 \n",
            "Epoch 20, Batch 200/611 \n",
            "Epoch 20, Batch 300/611 \n",
            "Epoch 20, Batch 400/611 \n",
            "Epoch 20, Batch 500/611 \n",
            "Epoch 20, Batch 600/611 \n",
            "Epoch 20/20, Loss: 1.6632\n",
            "Training completed.\n",
            "Input: عمرك رايح المكسيك؟\n",
            "Translated: I'm going to take a <unk>\n",
            "Chunk 3 completed: Training and inference implemented.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio interface\n",
        "def gradio_translate(text):\n",
        "    return translate(model, text)\n",
        "\n",
        "# Create Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=gradio_translate,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter Arabic sentence here...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Arabic to English Translator\",\n",
        "    description=\"Translate Arabic sentences to English using a Transformer model.\"\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "interface.launch()\n",
        "\n",
        "print(\"Chunk 4 completed: Gradio interface deployed.\")"
      ],
      "metadata": {
        "id": "9Nr_qAIFyGMP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}